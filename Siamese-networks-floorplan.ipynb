{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning with Siamese Networks\n",
    "\n",
    "This is the jupyter notebook that accompanies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All the imports are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Set of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img1, img2):\n",
    "    img1 = TF.to_pil_image(img1, mode='L')\n",
    "    img2 = TF.to_pil_image(img2, mode='L')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img2)\n",
    "    plt.show()    \n",
    "    \n",
    "def show_plot(train_loss, val_loss):\n",
    "    plt.plot(train_loss, label='train_loss')\n",
    "    plt.plot(val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def show_pred(test_results_0, test_results_1):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(test_results_0, label='pred distance of similar floorplans')\n",
    "    plt.plot(test_results_1, label='pred distance of different floorplans')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class\n",
    "A simple class to manage configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    train_file = './train_unit_ids.txt'\n",
    "    val_file = './val_unit_ids.txt'\n",
    "    test_file = './test_unit_ids.txt'\n",
    "    \n",
    "    data_dir = './pair_madori'\n",
    "    checkpoint_dir = './checkpoint'\n",
    "    \n",
    "    batch_size = 16\n",
    "    train_number_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "This dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MadoriDataset(Dataset):\n",
    "    \n",
    "    def _prepare(self):\n",
    "        if self.test:\n",
    "            data_file = Config.test_file\n",
    "        else:\n",
    "            data_file = Config.train_file if self.train else Config.val_file\n",
    "        with open(data_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        f.close()\n",
    "        self.unit_paths = [os.path.join(Config.data_dir, line.strip()) for line in lines]\n",
    "        \n",
    "    def __init__(self, img_size=(256, 256), train=True, test=False):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.img_size = (256, 256)\n",
    "        self._prepare()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.unit_paths)\n",
    "    \n",
    "    def _resize(self, img):\n",
    "        w, h = img.size\n",
    "        if w < h:\n",
    "            a = 256.0 / h\n",
    "            b = int(w * a)\n",
    "            img = img.resize((b, 256), Image.BILINEAR)\n",
    "        else:\n",
    "            a = 256.0 / w\n",
    "            b = int(h * a)\n",
    "            img = img.resize((256, b), Image.BILINEAR)\n",
    "        return img\n",
    "    \n",
    "    def _pad(self, img):\n",
    "        w, h = img.size\n",
    "        img = TF.pad(img, (0,0,256-w,0), padding_mode='edge') if h == 256 else \\\n",
    "               TF.pad(img, (0,0,0,256-h), padding_mode='edge')\n",
    "        \n",
    "        if img.size != self.img_size:\n",
    "            print('|-'*20, img.size, (w, h)) # (97, 256) (97, 249)\n",
    "        return img\n",
    "    \n",
    "    def _transform(self, img):\n",
    "        return self._pad(self._resize(img))\n",
    "    \n",
    "    def _aug_img(self, image):\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.rotate(image, random.choice([90, 180, 270]))\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "        return image\n",
    "    \n",
    "    def _select_img_path(self, unit_path, not_equal_to=None):\n",
    "        img_path = os.path.join(unit_path, random.choice(os.listdir(unit_path)))\n",
    "        if not_equal_to:\n",
    "            while img_path == not_equal_to:\n",
    "                img_path = os.path.join(unit_path, random.choice(os.listdir(unit_path)))\n",
    "        return img_path\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        unit_path1 = self.unit_paths[idx]\n",
    "        \n",
    "        img1_path = self._select_img_path(unit_path1)\n",
    "        img1 = self._transform(Image.open(img1_path).convert('L'))\n",
    "        \n",
    "        label = random.randint(0, 1)\n",
    "        if label:\n",
    "            # choose different floorplan\n",
    "            unit_path2 = unit_path1\n",
    "            while unit_path2 == unit_path1:\n",
    "                unit_path2 = random.choice(self.unit_paths)            \n",
    "            img2 = self._transform(Image.open(self._select_img_path(unit_path2)).convert('L'))\n",
    "        else:\n",
    "            # choose similar floorplan by augmentation\n",
    "            img2_path = self._select_img_path(unit_path1, not_equal_to=img1_path)\n",
    "            img2 = self._transform(Image.open(img2_path).convert('L'))\n",
    "            \n",
    "        img1, img2 = TF.to_tensor(self._aug_img(img1)), TF.to_tensor(self._aug_img(img2))\n",
    "        return img1, img2, torch.from_numpy(np.array([label],dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising some of the data\n",
    "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n",
    "1 indiciates dissimilar, and 0 indicates similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loader = DataLoader(MadoriDataset(), \n",
    "                           batch_size=Config.batch_size, \n",
    "                           shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dset_loader):\n",
    "    img1, img2, label = batch\n",
    "    print(img1.size(), img2.size())\n",
    "    for k in range(3):\n",
    "        print(label[k])\n",
    "        imshow(img1[k], img2[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Net Definition\n",
    "We will use a standard convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding=1, batch_norm=True):\n",
    "        super(ResConvBlock, self).__init__()\n",
    "\n",
    "        if batch_norm:\n",
    "            bn = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding))\n",
    "        self.bn1 = bn(out_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding))\n",
    "        self.bn2 = bn(out_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding))\n",
    "        self.bn3 = bn(out_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv2(x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        self.resnet1 = ResConvBlock(1, 8)\n",
    "        self.resnet2 = ResConvBlock(8, 8)\n",
    "        \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(1, 4, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8*256*256, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 5))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        #output = self.resnet1(x)\n",
    "        #output = self.resnet2(output)\n",
    "        \n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=5.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - \n",
    "                                        euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(MadoriDataset(), \n",
    "                                shuffle=True,\n",
    "                                batch_size=Config.batch_size)\n",
    "\n",
    "val_dataloader = DataLoader(MadoriDataset(train=False), \n",
    "                                shuffle=False,\n",
    "                                batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.0005 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, val_loss_history = [], []\n",
    "lowest_epoch_train_loss = lowest_epoch_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(Config.train_number_epochs)):\n",
    "    # training\n",
    "    net.train()\n",
    "    epoch_train_loss = 0\n",
    "    for batch_no, data in enumerate(train_dataloader):\n",
    "        img0, img1, label = data\n",
    "        img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = net(img0,img1)\n",
    "        batch_train_loss = criterion(output1,output2,label)\n",
    "        epoch_train_loss += batch_train_loss.item()\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= (batch_no + 1)\n",
    "    if epoch_train_loss < lowest_epoch_train_loss:\n",
    "        lowest_epoch_train_loss = epoch_train_loss\n",
    "        torch.save(net.state_dict(), f'{Config.checkpoint_dir}/best_train.pth')\n",
    "    train_loss_history.append(epoch_train_loss)\n",
    "    \n",
    "    # validation\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = 0\n",
    "        for batch_no, data in enumerate(val_dataloader):\n",
    "            img0, img1, label = data\n",
    "            img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n",
    "            output1,output2 = net(img0,img1)\n",
    "            batch_val_loss = criterion(output1,output2,label)\n",
    "            epoch_val_loss += batch_val_loss.item()\n",
    "        epoch_val_loss /= (batch_no + 1)\n",
    "        if epoch_val_loss < lowest_epoch_val_loss:\n",
    "            lowest_epoch_val_loss = epoch_val_loss\n",
    "            torch.save(net.state_dict(), f'{Config.checkpoint_dir}/best_val.pth')\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        \n",
    "    print(f'Epoch {epoch} training loss {epoch_train_loss}, validation loss {epoch_val_loss}')\n",
    "\n",
    "df = pd.DataFrame({'train_loss': train_loss_history, 'val_loss': val_loss_history})\n",
    "df.to_csv('./train_val_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:].plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "epoch_test = 0\n",
    "net = SiameseNetwork().to(device)\n",
    "net.load_state_dict(torch.load(f'{Config.checkpoint_dir}/best_val.pth'))\n",
    "test_dataloader = DataLoader(MadoriDataset(Config.test_file, test=True), \n",
    "                             shuffle=False, \n",
    "                             batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_results_0 = []\n",
    "test_results_1 = []\n",
    "with torch.no_grad():\n",
    "    for batch_no, data in enumerate(test_dataloader):\n",
    "        #if batch_no > 10: break\n",
    "        img0, img1, label = data\n",
    "        img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n",
    "        output1,output2 = net(img0,img1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        \n",
    "        img0, img1 = img0.cpu(), img1.cpu()\n",
    "        print(f'label: {label.item()}, pred distance: {euclidean_distance.item()}')\n",
    "        if label.item() == 1.0:\n",
    "            test_results_1 += [euclidean_distance.item()]\n",
    "        else:\n",
    "            test_results_0 += [euclidean_distance.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred(test_results_0, test_results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
